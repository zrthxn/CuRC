{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "651c41d9-420a-4e96-af86-0ccfa1fcd7b2",
   "metadata": {},
   "source": [
    "### <center><font color=navy> Tutorial #5 Computer- and robot-assisted surgery</font></center>\n",
    "## <center><font color=navy> Computer Vision Basics II</font></center>\n",
    "<center>&copy; Sebastian Bodenstedt, National Center for Tumor Diseases (NCT) Dresden<br>\n",
    "    <a href=\"https://www.nct-dresden.de/\"><img src=\"https://www.nct-dresden.de/++theme++nct/images/logo-nct-en.svg\"></a> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2a39e8-11e2-4924-9b39-76590ee4a258",
   "metadata": {},
   "source": [
    "## <center><font color=navy>Preperation</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f6b125-ddac-45d1-9bca-b1ff36588acf",
   "metadata": {},
   "source": [
    "For this tutorial, we will utilize the OpenCV, Matplotlib and NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb072f65-e316-4971-af26-99319c4584c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "# Force Matplotlib to display data directly in Jupyter\n",
    "%matplotlib inline \n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064408b2-2803-49b3-ac0c-c2e2f190d735",
   "metadata": {},
   "source": [
    "We will also download and extract a few image sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad1f45c-9c7a-4d5f-a3a9-32e76298d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from os.path import basename, exists\n",
    "import zipfile\n",
    "\n",
    "def download_and_extract(url): #download and extract Zip archive\n",
    "    file_path = basename(url)\n",
    "    if not exists(file_path): # does zip file already exist?\n",
    "        urllib.request.urlretrieve(url, file_path) # if not, download it\n",
    "        with zipfile.ZipFile(file_path, 'r') as zip_ref: # and unzip it\n",
    "            zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f94bfb6-1886-4889-ac54-095702a2093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download_and_extract(\"http://tso.ukdd.de/crs/Exercise1.zip\") # In case you didn't download the data last week\n",
    "download_and_extract(\"http://tso.ukdd.de/crs/CVBasicsII_chessboard.zip\")\n",
    "download_and_extract(\"http://tso.ukdd.de/crs/CVBasicsII_sequence1.zip\")\n",
    "download_and_extract(\"http://tso.ukdd.de/crs/CVBasicsII_sequence2.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da43280-1dfc-4da7-847f-6d1fc6b29ad3",
   "metadata": {},
   "source": [
    "We now list the extracted files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4538ff-1119-4562-9395-7a9f279d37f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6e2ce0-3e57-4585-b6f6-19f26f5fee66",
   "metadata": {},
   "source": [
    "## <center><font color=navy>Review</font></center>\n",
    "We can utilize OpenCV to read one of the images from HD and NumPy to process the data. And Numpy can be used for visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c5ea7a-ce82-4490-a5e8-c771a0404c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"Exercise1/img_01_raw.png\") # Read image from HD\n",
    "\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Convert BGR to RGB\n",
    "\n",
    "plt.imshow(img_rgb) # Display result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f9dffd-36bf-43e8-85f6-dd022d01bfec",
   "metadata": {},
   "source": [
    "Images can also be converted to grayscale and displayed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a02456b-295c-4fbc-8ae9-f860268c66b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_gray(img, canvas=plt, title=\"\"): # Later we want to draw on a different underground, so we define this as a parameter\n",
    "    canvas.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "    if not title == \"\":\n",
    "        canvas.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29d91ed-48e9-483b-b189-cbb67558810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Convert BGR to grayscale\n",
    "\n",
    "show_gray(img_gray,) # Display results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3555b460-ebb8-40ff-bf54-28187792a393",
   "metadata": {},
   "source": [
    "## <center><font color=navy>Filtering with OpenCV</font></center>\n",
    "Attempt to implement your own convolutional filter for grayscale images, for a custom kernel, e.g. Box, Prewitt, Sobel, Laplace, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492c2add-cf07-4576-881e-6075a1d56755",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_box = np.ones((3,3), dtype=np.float32)\n",
    "kernel_box/=9\n",
    "\n",
    "print(kernel_box)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ba17b7-2688-4e97-9b31-d00cbf62eda2",
   "metadata": {},
   "source": [
    "Or Gaussian filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c42d3b-41a9-446d-a564-80438f94aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_filter(r, muu=0, sigma=1):\n",
    "    x, y = np.meshgrid(np.linspace(-1, 1, 2*r+1),\n",
    "                       np.linspace(-1, 1, 2*r+1))\n",
    "    dst = np.sqrt(x**2+y**2)\n",
    " \n",
    "    normal = 1/(2 * np.pi * sigma**2)\n",
    "    gauss = np.exp(-((dst-muu)**2 / (2.0 * sigma**2))) * normal\n",
    "    \n",
    "    gauss/= np.sum(gauss)\n",
    "    \n",
    "    return gauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b50ebf-72a6-4779-8bdd-21774b6b9c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_gaussian = gaussian_filter(2, 0, 0.625)\n",
    "print(kernel_gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47178f57-e019-4ebe-be8a-39a3a3457d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kernel(image, kernel):\n",
    "    assert kernel.shape[0] == kernel.shape[1]\n",
    "    assert kernel.shape[0] % 2 == 1\n",
    "    \n",
    "    r = (kernel.shape[0] - 1)//2\n",
    "    \n",
    "    #TODO Apply filter\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa1f067-8911-46c3-a1d6-36d965f1fc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = apply_kernel(img_gray, kernel_prewitt_x)\n",
    "print(np.min(result), np.max(result), result.dtype)\n",
    "\n",
    "figure, axis = plt.subplots(1, 2, figsize=(15, 15)) # subplots let you visualize multiple outputs simultanously\n",
    "\n",
    "show_gray(img_gray, axis[0])\n",
    "show_gray(result, axis[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a542a1e-519b-48a1-9cf9-41758907e642",
   "metadata": {},
   "source": [
    "OpenCV has implement its own convolutional filtering method (higher performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4f9d18-8f13-4693-9171-ea80940f21ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axis = plt.subplots(1, 2, figsize=(15, 15)) # subplots let you visualize multiple outputs simultanously\n",
    "\n",
    "result_cv = cv2.filter2D(img_gray, kernel=kernel_prewitt_x, ddepth=-1)\n",
    "\n",
    "show_gray(img_gray, axis[0])\n",
    "show_gray(result_cv, axis[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cbe71c-b6af-4775-ad0d-7835df6cd525",
   "metadata": {},
   "source": [
    "The OpenCV also contains implementations for the Median Filter and the Bilateral Filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cff9f3-9d7f-4651-acb8-4cfed206853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axis = plt.subplots(1, 2, figsize=(15, 15)) # subplots let you visualize multiple outputs simultanously\n",
    "\n",
    "result_cv = cv2.medianBlur(img_gray, 31) # Median Filter over 3x3 neighborhood\n",
    "\n",
    "show_gray(img_gray, axis[0])\n",
    "show_gray(result_cv, axis[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ccd506-b13c-4ea0-8c85-a88347262a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axis = plt.subplots(1, 2, figsize=(15, 15)) # subplots let you visualize multiple outputs simultanously\n",
    "\n",
    "result_cv = cv2.bilateralFilter(img_rgb, d=-1, sigmaColor=51, sigmaSpace=51)\n",
    "\n",
    "show_gray(img_gray, axis[0])\n",
    "show_gray(result_cv, axis[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82885f79-e641-4b51-90f1-24b2cd9e3a27",
   "metadata": {},
   "source": [
    "Implement a few other filter matrices and try out the different filters and combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3957fa9f-c206-417c-9df2-2f31bb14d5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axis = plt.subplots(1, 2, figsize=(30, 30)) # subplots let you visualize multiple outputs simultanously\n",
    "\n",
    "show_gray(img_gray, axis[0, 0], \"Original\")\n",
    "result_cv = cv2.filter2D(img_gray, kernel=kernel_box, ddepth=-1)\n",
    "show_gray(result_cv, axis[0, 1], \"Box Filter\")\n",
    "result_cv = cv2.filter2D(img_gray, kernel=kernel_gaussian, ddepth=-1)\n",
    "\n",
    "# TODO: Try out different filters and combinations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0195b0-38b1-4510-8137-f33242fe3573",
   "metadata": {},
   "source": [
    "## <center><font color=navy>Stereo-camera calibration with OpenCV</font></center>\n",
    "OpenCV also provides functionality for loading image sequences and videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178c18e2-40c1-4c3b-9d2c-ab1ab4392cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"CVBasicsII/chessboard/scene_left%04d.png\") # Load image sequence\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done: # Iterate over the sequence until finished\n",
    "    ret, frame = cap.read()\n",
    "    done = not ret\n",
    "    \n",
    "    if ret:\n",
    "        cv2.imshow(\"Video\", frame) # Display frame\n",
    "        cv2.waitKey(100)\n",
    "    \n",
    "cv2.destroyAllWindows() # Very important! Otherwise, Jupyter will hang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c257b9c-2bbe-4152-9aa4-26f6fa5a5f8e",
   "metadata": {},
   "source": [
    "Traditionally images of chessboard patterns are used for calibration. OpenCV contains functionality for detecting and drawing chessboard patterns. First we need to define the size of the pattern. In our case we have 17 columns and 12 rows, with 5x5 mm squares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5871aa59-8592-499b-8924-6abd356ff922",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 17\n",
    "rows = 12\n",
    "sqrsize = 5\n",
    "\n",
    "flags = cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_NORMALIZE_IMAGE + cv2.CALIB_CB_FAST_CHECK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6ade34-8ae2-4427-a185-6829db6a24f0",
   "metadata": {},
   "source": [
    "We can then detected the pattern in images using OpenCV and draw them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f111fa-11fe-45d1-96b1-0d651e0e044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"CVBasicsII/chessboard/scene_left0000.png\")\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # The image has to be converted to grayscale\n",
    "\n",
    "retval, corners = cv2.findChessboardCorners(img_gray, (cols-1, rows-1), flags=flags)\n",
    "\n",
    "img_corners = cv2.drawChessboardCorners(img, (cols-1,rows-1), corners, retval)\n",
    "\n",
    "plt.imshow(img_corners)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd6560f-7826-42ae-bfcf-85e1fa519f13",
   "metadata": {},
   "source": [
    "We can refine these to subpixel level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b4bf83-72a4-4cb7-abc7-cacc9b12ba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "\n",
    "corners_sub = cv2.cornerSubPix(img_gray, corners, (5, 5), (-1, -1), criteria)\n",
    "\n",
    "img_corners = cv2.drawChessboardCorners(img, (cols-1,rows-1), corners_sub, retval)\n",
    "\n",
    "plt.imshow(img_corners)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d318cf3c-fe35-40d3-a728-bd9ec449a114",
   "metadata": {},
   "source": [
    "We can now combine the locating of the corner pixels with reading image sequences, to locate chessboard pattern in the left (scene_left%04d.png) and right (scene_right%04d.png) sequences and save them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ca4f4-f62d-4290-8353-f7a63513f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_left = cv2.VideoCapture(\"CVBasicsII/chessboard/scene_left%04d.png\") # Load image sequence left\n",
    "cap_right = cv2.VideoCapture(\"CVBasicsII/chessboard/scene_right%04d.png\") # Load image sequence right\n",
    "\n",
    "corners_left = [] # Save corners\n",
    "corners_right = []\n",
    "width = 0\n",
    "height = 0\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done: # Iterate over the sequence until finished\n",
    "    # TODO Iterate through all images and save patterns if corresponding left and right images both contain one at the current time point. Also determine width and height of image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf37ed3-c7cc-46ad-ba15-7787448ba878",
   "metadata": {},
   "source": [
    "Let's check how many patterns we found in both images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb76fe86-0623-4ae7-a0cf-c6b23bb12097",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_corners_left), len(all_corners_right))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25145140-5868-4d9c-92e6-5980cd518874",
   "metadata": {},
   "source": [
    "To calibrate the stereo camera system, we need to define the 3D-geometry of the pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df225249-ad88-4f1e-b7e4-baf21c30b93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "objp = np.zeros((1, (rows-1)*(cols-1), 3), np.float32)\n",
    "objp[0,:,:2] = np.mgrid[0:cols-1, 0:rows-1].T.reshape(-1, 2)*sqrsize\n",
    "print(objp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97d9a70-75da-4b22-b296-9d8609606698",
   "metadata": {},
   "source": [
    "Furthermore, we need to match a 3D-pattern to each located 2D-Pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09563a9-52b1-4489-8953-7c081ff8e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "objpoints = []\n",
    "for i in range(len(all_corners_left)):\n",
    "    objpoints.append(objp) # Each image shows the same pattern (just in different poses), we can therefore use the same 3D pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca6fd15-b0c2-44d1-a358-04e6909e1589",
   "metadata": {},
   "source": [
    "Now we are already ready to calibrate our stereo camera system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b7f1ea-4846-486b-8e72-cbd5dd33ddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stereocalibration_flags = cv2.CALIB_FIX_K3\n",
    "stereocalibration_criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 50, 0.000001)\n",
    "\n",
    "ret, K1, d1, K2, d2, R, T, E, F = cv2.stereoCalibrate(objpoints, all_corners_left, all_corners_right, None, None, None, None, (width, height), criteria=stereocalibration_criteria, flags=stereocalibration_flags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95789ed5-3df4-4aee-912d-91f33f7a0bc7",
   "metadata": {},
   "source": [
    "We should now have computed sensible values for the camera matrices, the distortions and the extrinsic parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b842654-1fde-4ed5-82d5-88974e8daefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(K1)\n",
    "print(K2)\n",
    "print(d1)\n",
    "print(d2)\n",
    "print(R)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe35e12-0bf6-4ce9-b47d-a29f907ea082",
   "metadata": {},
   "source": [
    "## <center><font color=navy>Stereo rectification</font></center>\n",
    "For stereo triangulation, we need to locate matching points in the two images. Here the epipolar geometry can help. Let's say, we have a point in the left image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89999035-b370-406c-843d-68ecbcc5755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_left = cv2.imread(\"CVBasicsII/sequence1/scene_left0000.png\")\n",
    "img_right = cv2.imread(\"CVBasicsII/sequence1/scene_right0000.png\")\n",
    "\n",
    "px = 110\n",
    "py = 208\n",
    "img_left[py-3:py+3,px-3:px+3, :] = 255\n",
    "\n",
    "figure, axis = plt.subplots(1, 2, figsize=(30, 30)) # subplots let you visualize multiple outputs simultanously\n",
    "\n",
    "axis[0].imshow(img_left[:, :, ::-1])\n",
    "axis[1].imshow(img_right[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73242b6-4bbe-4451-a963-79e9433fed71",
   "metadata": {},
   "source": [
    "We can narrow the search space using epipolar geometry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ad7be4-ea62-454b-a64c-2c633a3060ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_l = np.asarray([px, py, 1])\n",
    "l_r = F@p_l # Returns an epipolar line in the right image\n",
    "print(l_r)\n",
    "a, b, c = l_r.ravel() # Get the 3 line parameters from the line\n",
    "\n",
    "x = np.array([0, img_right.shape[1]]) # Calculate y-coordinates for 0 and image width\n",
    "y = -(x*a + c) / b\n",
    "print(x, y)\n",
    "y = y.astype(np.int64)\n",
    "cv2.line(img_right, (x[0], y[0]), (x[1], y[1]), (255, 255, 255), thickness=2) # Draw line into the image\n",
    "\n",
    "figure, axis = plt.subplots(1, 2, figsize=(30, 30)) # subplots let you visualize multiple outputs simultanously\n",
    "\n",
    "axis[0].imshow(img_left[:, :, ::-1])\n",
    "axis[1].imshow(img_right[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869b43b2-bb44-4721-b65d-46d86dbd25db",
   "metadata": {},
   "source": [
    "Rectification can make locating matches faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5bcc38-8f26-4fe5-b156-e2ea17c11898",
   "metadata": {},
   "outputs": [],
   "source": [
    "R1, R2, P1, P2, Q, roi_left, roi_right = cv2.stereoRectify(K1, d1, K2, d2, (width, height), R, T, flags=cv2.CALIB_ZERO_DISPARITY, alpha=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e51b9fd-a436-4e8e-ab2e-0bc98475f906",
   "metadata": {},
   "source": [
    "We can use these parameters to directly rectify images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44382fde-fe84-4e77-9082-5fb8d61c1156",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "map1x, map1y = cv2.initUndistortRectifyMap(K1, d1, R1, P1, (width, height), cv2.CV_32FC1)\n",
    "map2x, map2y = cv2.initUndistortRectifyMap(K2, d2, R2, P2, (width, height), cv2.CV_32FC1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d648ac02-06f2-4243-8437-5d7764ceb54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_rec = cv2.remap(img_left, map1x, map1y, cv2.INTER_LINEAR)\n",
    "right_rec = cv2.remap(img_right, map2x, map2y, cv2.INTER_LINEAR)\n",
    "\n",
    "figure, axis = plt.subplots(1, 2, figsize=(30, 30)) # subplots let you visualize multiple outputs simultanously\n",
    "\n",
    "axis[0].imshow(left_rec[:, :, ::-1])\n",
    "axis[1].imshow(right_rec[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c41f33-5d2a-4742-ada2-d7f1444a10c3",
   "metadata": {},
   "source": [
    "### <center><font color=navy>3D reconstruction</font></center>\n",
    "We can use the rectified images now as input for the semi-global block matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdee993f-13fd-4a33-b22b-6eb8d1b9a420",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_left = cv2.imread(\"CVBasicsII/sequence1/scene_left0015.png\")\n",
    "img_right = cv2.imread(\"CVBasicsII/sequence1/scene_right0015.png\")\n",
    "\n",
    "left_rec = cv2.remap(img_left, map1x, map1y, cv2.INTER_LINEAR)\n",
    "right_rec = cv2.remap(img_right, map2x, map2y, cv2.INTER_LINEAR)\n",
    "\n",
    "minDisparity = 10\n",
    "maxDisparity = 192\n",
    "blocksize = 3\n",
    "\n",
    "p1 = 8*3*blocksize*blocksize\n",
    "p2 = 32*3*blocksize*blocksize\n",
    "\n",
    "stereoProcessor = cv2.StereoSGBM_create(minDisparity, maxDisparity, blocksize, p1, p2)\n",
    "\n",
    "disp = stereoProcessor.compute(left_rec, right_rec)\n",
    "\n",
    "figure, axis = plt.subplots(1, 2, figsize=(30, 30)) # subplots let you visualize multiple outputs simultanously\n",
    "\n",
    "axis[0].imshow(left_rec[:, :, ::-1])\n",
    "axis[1].imshow(disp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba7f9d7-2718-4ec9-90cc-f3e0bc9b9666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Visualize/reconstruct entire video"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
