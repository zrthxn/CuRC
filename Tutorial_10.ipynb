{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "651c41d9-420a-4e96-af86-0ccfa1fcd7b2",
   "metadata": {
    "id": "651c41d9-420a-4e96-af86-0ccfa1fcd7b2"
   },
   "source": [
    "### <center><font color=navy> Tutorial #10 Computer- and robot-assisted surgery</font></center>\n",
    "## <center><font color=navy> Segmentation III/ML Basics II</font></center>\n",
    "<center>&copy; Sebastian Bodenstedt, National Center for Tumor Diseases (NCT) Dresden<br>\n",
    "    <a href=\"https://www.nct-dresden.de/\"><img src=\"https://www.nct-dresden.de/++theme++nct/images/logo-nct-en.svg\"></a> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2a39e8-11e2-4924-9b39-76590ee4a258",
   "metadata": {
    "id": "ad2a39e8-11e2-4924-9b39-76590ee4a258"
   },
   "source": [
    "## <center><font color=navy>Preperation</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f6b125-ddac-45d1-9bca-b1ff36588acf",
   "metadata": {
    "id": "36f6b125-ddac-45d1-9bca-b1ff36588acf"
   },
   "source": [
    "For this tutorial, we will utilize the OpenCV, Matplotlib and NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb072f65-e316-4971-af26-99319c4584c1",
   "metadata": {
    "id": "eb072f65-e316-4971-af26-99319c4584c1"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064408b2-2803-49b3-ac0c-c2e2f190d735",
   "metadata": {
    "id": "064408b2-2803-49b3-ac0c-c2e2f190d735"
   },
   "source": [
    "We will also download some data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa21f52-5e53-4f44-bb3e-7607c7c41692",
   "metadata": {
    "id": "ffa21f52-5e53-4f44-bb3e-7607c7c41692"
   },
   "source": [
    "## <center><font color=navy>Simple neural network example</font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ad1f45c-9c7a-4d5f-a3a9-32e76298d5ce",
   "metadata": {
    "id": "4ad1f45c-9c7a-4d5f-a3a9-32e76298d5ce"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from os.path import basename, exists\n",
    "\n",
    "def download(url): #download and extract Zip archive\n",
    "    file_path = basename(url)\n",
    "    if not exists(file_path): # does zip file already exist?\n",
    "        urllib.request.urlretrieve(url, file_path) # if not, download it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f94bfb6-1886-4889-ac54-095702a2093f",
   "metadata": {
    "id": "7f94bfb6-1886-4889-ac54-095702a2093f"
   },
   "outputs": [],
   "source": [
    "download(\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\")\n",
    "download(\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names\")\n",
    "download(\"http://tso.ukdd.de/crs/model0049.th\")\n",
    "download(\"http://tso.ukdd.de/crs/img1.png\")\n",
    "download(\"http://tso.ukdd.de/crs/img2.png\")\n",
    "download(\"http://tso.ukdd.de/crs/img3.png\")\n",
    "download(\"http://tso.ukdd.de/crs/img4.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ba17b7-2688-4e97-9b31-d00cbf62eda2",
   "metadata": {
    "id": "20ba17b7-2688-4e97-9b31-d00cbf62eda2"
   },
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "492c2add-cf07-4576-881e-6075a1d56755",
   "metadata": {
    "id": "492c2add-cf07-4576-881e-6075a1d56755"
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f78f4c6-a6a8-475f-9af8-228956486edd",
   "metadata": {
    "id": "3f78f4c6-a6a8-475f-9af8-228956486edd"
   },
   "source": [
    "Show the readme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e15be1a8-8af3-466a-8641-efe155ba03ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e15be1a8-8af3-466a-8641-efe155ba03ba",
    "outputId": "2389f76e-ee54-44b5-b393-7ad88e71d107"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Title: Pima Indians Diabetes Database\n",
      "\n",
      "2. Sources:\n",
      "   (a) Original owners: National Institute of Diabetes and Digestive and\n",
      "                        Kidney Diseases\n",
      "   (b) Donor of database: Vincent Sigillito (vgs@aplcen.apl.jhu.edu)\n",
      "                          Research Center, RMI Group Leader\n",
      "                          Applied Physics Laboratory\n",
      "                          The Johns Hopkins University\n",
      "                          Johns Hopkins Road\n",
      "                          Laurel, MD 20707\n",
      "                          (301) 953-6231\n",
      "   (c) Date received: 9 May 1990\n",
      "\n",
      "3. Past Usage:\n",
      "    1. Smith,~J.~W., Everhart,~J.~E., Dickson,~W.~C., Knowler,~W.~C., \\&\n",
      "       Johannes,~R.~S. (1988). Using the ADAP learning algorithm to forecast\n",
      "       the onset of diabetes mellitus.  In {\\it Proceedings of the Symposium\n",
      "       on Computer Applications and Medical Care} (pp. 261--265).  IEEE\n",
      "       Computer Society Press.\n",
      "\n",
      "       The diagnostic, binary-valued variable investigated is whether the\n",
      "       patient shows signs of diabetes according to World Health Organization\n",
      "       criteria (i.e., if the 2 hour post-load plasma glucose was at least \n",
      "       200 mg/dl at any survey  examination or if found during routine medical\n",
      "       care).   The population lives near Phoenix, Arizona, USA.\n",
      "\n",
      "       Results: Their ADAP algorithm makes a real-valued prediction between\n",
      "       0 and 1.  This was transformed into a binary decision using a cutoff of \n",
      "       0.448.  Using 576 training instances, the sensitivity and specificity\n",
      "       of their algorithm was 76% on the remaining 192 instances.\n",
      "\n",
      "4. Relevant Information:\n",
      "      Several constraints were placed on the selection of these instances from\n",
      "      a larger database.  In particular, all patients here are females at\n",
      "      least 21 years old of Pima Indian heritage.  ADAP is an adaptive learning\n",
      "      routine that generates and executes digital analogs of perceptron-like\n",
      "      devices.  It is a unique algorithm; see the paper for details.\n",
      "\n",
      "5. Number of Instances: 768\n",
      "\n",
      "6. Number of Attributes: 8 plus class \n",
      "\n",
      "7. For Each Attribute: (all numeric-valued)\n",
      "   1. Number of times pregnant\n",
      "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
      "   3. Diastolic blood pressure (mm Hg)\n",
      "   4. Triceps skin fold thickness (mm)\n",
      "   5. 2-Hour serum insulin (mu U/ml)\n",
      "   6. Body mass index (weight in kg/(height in m)^2)\n",
      "   7. Diabetes pedigree function\n",
      "   8. Age (years)\n",
      "   9. Class variable (0 or 1)\n",
      "\n",
      "8. Missing Attribute Values: Yes\n",
      "\n",
      "9. Class Distribution: (class value 1 is interpreted as \"tested positive for\n",
      "   diabetes\")\n",
      "\n",
      "   Class Value  Number of instances\n",
      "   0            500\n",
      "   1            268\n",
      "\n",
      "10. Brief statistical analysis:\n",
      "\n",
      "    Attribute number:    Mean:   Standard Deviation:\n",
      "    1.                     3.8     3.4\n",
      "    2.                   120.9    32.0\n",
      "    3.                    69.1    19.4\n",
      "    4.                    20.5    16.0\n",
      "    5.                    79.8   115.2\n",
      "    6.                    32.0     7.9\n",
      "    7.                     0.5     0.3\n",
      "    8.                    33.2    11.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(\"pima-indians-diabetes.names\", \"r\")\n",
    "lines = f.readlines()\n",
    "for l in lines:\n",
    "    print(l[:-1])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eff8e8-00e6-4c1c-bbdc-67ac5c7fd226",
   "metadata": {
    "id": "b2eff8e8-00e6-4c1c-bbdc-67ac5c7fd226"
   },
   "source": [
    "Let's divide the dataset into training and testing batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f8231e7-52c8-4426-8892-31d35750bf2f",
   "metadata": {
    "id": "5f8231e7-52c8-4426-8892-31d35750bf2f"
   },
   "outputs": [],
   "source": [
    "x = data[:, :8]\n",
    "y = data[:, 8]\n",
    "\n",
    "x_train = x[:700].astype(np.float32)\n",
    "y_train = y[:700].astype(np.float32)\n",
    "\n",
    "x_test = x[700:].astype(np.float32)\n",
    "y_test = y[700:].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c9fac4-a77f-4cd8-80c8-7c3ef6714efd",
   "metadata": {
    "id": "83c9fac4-a77f-4cd8-80c8-7c3ef6714efd"
   },
   "source": [
    "Now, let's construct a simple neural network to classify the samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9e9202e-03ef-4210-a633-405fb5d1791f",
   "metadata": {
    "id": "e9e9202e-03ef-4210-a633-405fb5d1791f"
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__() # Call constructur of parent class\n",
    "        \n",
    "        self.fc1 = nn.Linear(8, 10) # Create a fully-connected (linear) input layer with 8 inputs and 10 outputs\n",
    "        #self.fc2 = nn.Linear(10, 10) # Create a fully-connected (linear) hidden layer with 10 inputs and 10 outputs\n",
    "        self.fc3 = nn.Linear(10, 1) # Create a fully-connected (linear) output layer with 10 inputs and 1 output\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        #x = self.fc2(x)\n",
    "        #x = torch.sigmoid(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zqYvKCLuAvTS",
   "metadata": {
    "id": "zqYvKCLuAvTS"
   },
   "source": [
    "Note: Other non-linearities than Sigmoid exist in pytorch, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dQ0F6XZPAukx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dQ0F6XZPAukx",
    "outputId": "4742db1a-a474-4111-eccc-68b861447a34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6410, 0.6459, 0.5232, 0.7270, 0.6021, 0.6343, 0.7014, 0.7021, 0.5309,\n",
      "         0.6463],\n",
      "        [0.5017, 0.6336, 0.5478, 0.6679, 0.6868, 0.6017, 0.5851, 0.6095, 0.6428,\n",
      "         0.5599],\n",
      "        [0.5123, 0.6883, 0.7168, 0.5025, 0.5978, 0.7063, 0.6925, 0.6400, 0.7091,\n",
      "         0.5201],\n",
      "        [0.5351, 0.5967, 0.6309, 0.7040, 0.6609, 0.6624, 0.5814, 0.7102, 0.6399,\n",
      "         0.6119],\n",
      "        [0.5424, 0.6467, 0.6002, 0.7230, 0.5328, 0.6380, 0.5616, 0.7012, 0.6274,\n",
      "         0.6448],\n",
      "        [0.6965, 0.6165, 0.7124, 0.6983, 0.5009, 0.6231, 0.5112, 0.6531, 0.5510,\n",
      "         0.6214],\n",
      "        [0.6826, 0.5240, 0.6604, 0.6978, 0.6204, 0.6361, 0.5953, 0.6323, 0.6789,\n",
      "         0.6065],\n",
      "        [0.5873, 0.5002, 0.5075, 0.7160, 0.7014, 0.6223, 0.6269, 0.6054, 0.5983,\n",
      "         0.7213],\n",
      "        [0.5393, 0.6411, 0.5234, 0.5507, 0.6618, 0.6099, 0.5729, 0.6246, 0.7147,\n",
      "         0.5159],\n",
      "        [0.6765, 0.5687, 0.7250, 0.6465, 0.5291, 0.6819, 0.5011, 0.7264, 0.6792,\n",
      "         0.5687]])\n",
      "tensor([[5.7973e-01, 6.0102e-01, 9.2795e-02, 9.7926e-01, 4.1426e-01, 5.5063e-01,\n",
      "         8.5418e-01, 8.5755e-01, 1.2386e-01, 6.0261e-01],\n",
      "        [6.8050e-03, 5.4785e-01, 1.9162e-01, 6.9854e-01, 7.8515e-01, 4.1250e-01,\n",
      "         3.4364e-01, 4.4504e-01, 5.8736e-01, 2.4066e-01],\n",
      "        [4.9368e-02, 7.9210e-01, 9.2885e-01, 9.9741e-03, 3.9625e-01, 8.7762e-01,\n",
      "         8.1197e-01, 5.7544e-01, 8.9083e-01, 8.0555e-02],\n",
      "        [1.4082e-01, 3.9165e-01, 5.3592e-01, 8.6643e-01, 6.6725e-01, 6.7397e-01,\n",
      "         3.2869e-01, 8.9657e-01, 5.7479e-01, 4.5515e-01],\n",
      "        [1.7004e-01, 6.0465e-01, 4.0629e-01, 9.5953e-01, 1.3139e-01, 5.6660e-01,\n",
      "         2.4769e-01, 8.5282e-01, 5.2088e-01, 5.9619e-01],\n",
      "        [8.3076e-01, 4.7462e-01, 9.0691e-01, 8.3914e-01, 3.6588e-03, 5.0281e-01,\n",
      "         4.4762e-02, 6.3279e-01, 2.0458e-01, 4.9540e-01],\n",
      "        [7.6562e-01, 9.6077e-02, 6.6528e-01, 8.3683e-01, 4.9130e-01, 5.5830e-01,\n",
      "         3.8574e-01, 5.4231e-01, 7.4867e-01, 4.3259e-01],\n",
      "        [3.5291e-01, 8.1712e-04, 3.0005e-02, 9.2484e-01, 8.5419e-01, 4.9950e-01,\n",
      "         5.1878e-01, 4.2781e-01, 3.9841e-01, 9.5102e-01],\n",
      "        [1.5760e-01, 5.8010e-01, 9.3487e-02, 2.0339e-01, 6.7128e-01, 4.4682e-01,\n",
      "         2.9375e-01, 5.0933e-01, 9.1808e-01, 6.3796e-02],\n",
      "        [7.3765e-01, 2.7645e-01, 9.6937e-01, 6.0387e-01, 1.1647e-01, 7.6256e-01,\n",
      "         4.2867e-03, 9.7640e-01, 7.4990e-01, 2.7656e-01]])\n",
      "tensor([[0.5225, 0.5378, 0.0925, 0.7527, 0.3921, 0.5010, 0.6932, 0.6950, 0.1232,\n",
      "         0.5389],\n",
      "        [0.0068, 0.4989, 0.1893, 0.6034, 0.6556, 0.3906, 0.3307, 0.4178, 0.5280,\n",
      "         0.2361],\n",
      "        [0.0493, 0.6596, 0.7301, 0.0100, 0.3767, 0.7052, 0.6707, 0.5193, 0.7118,\n",
      "         0.0804],\n",
      "        [0.1399, 0.3728, 0.4899, 0.6996, 0.5832, 0.5876, 0.3173, 0.7146, 0.5189,\n",
      "         0.4261],\n",
      "        [0.1684, 0.5403, 0.3853, 0.7441, 0.1306, 0.5129, 0.2427, 0.6925, 0.4784,\n",
      "         0.5343],\n",
      "        [0.6809, 0.4419, 0.7196, 0.6854, 0.0037, 0.4643, 0.0447, 0.5600, 0.2018,\n",
      "         0.4585],\n",
      "        [0.6444, 0.0958, 0.5819, 0.6841, 0.4552, 0.5067, 0.3677, 0.4947, 0.6344,\n",
      "         0.4075],\n",
      "        [0.3390, 0.0008, 0.0300, 0.7282, 0.6933, 0.4617, 0.4768, 0.4035, 0.3786,\n",
      "         0.7402],\n",
      "        [0.1563, 0.5227, 0.0932, 0.2006, 0.5858, 0.4193, 0.2856, 0.4694, 0.7250,\n",
      "         0.0637],\n",
      "        [0.6277, 0.2696, 0.7484, 0.5398, 0.1159, 0.6426, 0.0043, 0.7515, 0.6351,\n",
      "         0.2697]])\n",
      "tensor([[0.0977, 0.0998, 0.0600, 0.1456, 0.0828, 0.0949, 0.1285, 0.1289, 0.0619,\n",
      "         0.0999],\n",
      "        [0.0641, 0.1101, 0.0771, 0.1280, 0.1396, 0.0962, 0.0898, 0.0994, 0.1146,\n",
      "         0.0810],\n",
      "        [0.0576, 0.1210, 0.1388, 0.0554, 0.0815, 0.1318, 0.1235, 0.0975, 0.1336,\n",
      "         0.0594],\n",
      "        [0.0646, 0.0830, 0.0959, 0.1335, 0.1093, 0.1101, 0.0779, 0.1375, 0.0997,\n",
      "         0.0885],\n",
      "        [0.0691, 0.1067, 0.0875, 0.1522, 0.0665, 0.1027, 0.0747, 0.1368, 0.0981,\n",
      "         0.1058],\n",
      "        [0.1338, 0.0937, 0.1444, 0.1350, 0.0585, 0.0964, 0.0610, 0.1098, 0.0716,\n",
      "         0.0957],\n",
      "        [0.1213, 0.0621, 0.1097, 0.1302, 0.0922, 0.0986, 0.0829, 0.0970, 0.1192,\n",
      "         0.0869],\n",
      "        [0.0824, 0.0580, 0.0597, 0.1460, 0.1361, 0.0954, 0.0973, 0.0888, 0.0863,\n",
      "         0.1499],\n",
      "        [0.0761, 0.1162, 0.0714, 0.0797, 0.1273, 0.1017, 0.0872, 0.1082, 0.1629,\n",
      "         0.0693],\n",
      "        [0.1147, 0.0723, 0.1446, 0.1003, 0.0616, 0.1176, 0.0551, 0.1456, 0.1161,\n",
      "         0.0723]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(10, 10)\n",
    "\n",
    "print(torch.sigmoid(x))\n",
    "print(torch.relu(x))\n",
    "print(torch.tanh(x))\n",
    "print(torch.softmax(x, dim=1)) # Generally only used for last layer in classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439fc584-55ef-4cde-bf9d-baf1a4be0e5d",
   "metadata": {
    "id": "439fc584-55ef-4cde-bf9d-baf1a4be0e5d"
   },
   "source": [
    "We divide the dataset in so-called \"Batches\". We perform the back-propagation step and the gradient descent step for each batch separately, reducing the memory footprint and speeding up calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4268c61-1e04-42ab-9a43-a3a3d625de6b",
   "metadata": {
    "id": "b4268c61-1e04-42ab-9a43-a3a3d625de6b"
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "\n",
    "num_train_batches = math.ceil(x_train.shape[0]/batch_size)\n",
    "num_test_batches = math.ceil(x_test.shape[0]/batch_size)\n",
    "\n",
    "x_train_batches = np.array_split(x_train, num_train_batches, 0)\n",
    "y_train_batches = np.array_split(y_train, num_train_batches, 0)\n",
    "\n",
    "x_test_batches = np.array_split(x_test, num_test_batches, 0)\n",
    "y_test_batches = np.array_split(y_test, num_test_batches, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821d3556-61ef-4997-96aa-6b308599bddb",
   "metadata": {
    "id": "821d3556-61ef-4997-96aa-6b308599bddb"
   },
   "source": [
    "Furthermore, we need to define a cost function. As we are performing binary classification, we will utilize cross-entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c5a139d-c8f5-40a8-8190-df0594c05825",
   "metadata": {
    "id": "8c5a139d-c8f5-40a8-8190-df0594c05825"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nP57lz8w-vlT",
   "metadata": {
    "id": "nP57lz8w-vlT"
   },
   "source": [
    "We also have to decide whether we could like to use the GPU or the CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "796084a1-26ef-4742-8a16-8206a0f90fe4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "796084a1-26ef-4742-8a16-8206a0f90fe4",
    "outputId": "092adbf1-939b-48da-9fc6-1b7a9b3c6691"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "use_gpu = True\n",
    "\n",
    "# device = torch.device(\"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "device = \"mps\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c9d532-df95-4b19-889c-8bfb5b04b908",
   "metadata": {
    "id": "37c9d532-df95-4b19-889c-8bfb5b04b908"
   },
   "source": [
    "We also initialize our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d957f2df-c752-48d1-b8c0-6a534a312fc6",
   "metadata": {
    "id": "d957f2df-c752-48d1-b8c0-6a534a312fc6"
   },
   "outputs": [],
   "source": [
    "model = Network()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d527612-8064-41e2-be23-1b5307b6191b",
   "metadata": {
    "id": "0d527612-8064-41e2-be23-1b5307b6191b"
   },
   "source": [
    "Next, we define the optimizer. The optimizer performs the gradient step for us. Here, we utilize so-called \"Stochastic Gradient Descent\". We also have to specify the step size for gradient descent, the so-called learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fc4627b-eddf-45f3-a2bb-720860bec6b7",
   "metadata": {
    "id": "5fc4627b-eddf-45f3-a2bb-720860bec6b7"
   },
   "outputs": [],
   "source": [
    "learning_rate = 5e-3\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dac7d5-54bf-46b8-bf6b-20235b9795ad",
   "metadata": {
    "id": "21dac7d5-54bf-46b8-bf6b-20235b9795ad"
   },
   "source": [
    "We then define a training function, which iterates through all batches, converts the data into pytorch data types, puts the data through the network and finally performs the backpropagation and gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d105237-cfdf-4d11-869d-20e860c5455e",
   "metadata": {
    "id": "1d105237-cfdf-4d11-869d-20e860c5455e"
   },
   "outputs": [],
   "source": [
    "def training_step(model, x, y, loss, optimizer):\n",
    "    model.train()\n",
    "    accuracy = 0\n",
    "    losses = []\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x_tensor = torch.from_numpy(x[i]).to(device)\n",
    "        y_tensor = torch.from_numpy(y[i]).to(device)\n",
    "        \n",
    "        output = model(x_tensor)\n",
    "        output = output.squeeze(1)\n",
    "        \n",
    "        loss = criterion(output, y_tensor)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        prediction = torch.round(output)\n",
    "        \n",
    "        accuracy += torch.sum(prediction == y_tensor).item()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        count += x_tensor.shape[0]\n",
    "        \n",
    "    return np.mean(losses), accuracy/count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e467c5-4bde-4102-abae-99648ef419b6",
   "metadata": {
    "id": "00e467c5-4bde-4102-abae-99648ef419b6"
   },
   "source": [
    "We also define a validation function that we can use to simply test the model on data, without modifying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cc57005-df1a-43d6-9fb6-b095cdb8c059",
   "metadata": {
    "id": "3cc57005-df1a-43d6-9fb6-b095cdb8c059"
   },
   "outputs": [],
   "source": [
    "def validation_step(model, x, y, loss):\n",
    "    model.eval()\n",
    "    accuracy = 0\n",
    "    losses = []\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x)):\n",
    "            x_tensor = torch.from_numpy(x[i]).to(device)\n",
    "            y_tensor = torch.from_numpy(y[i]).to(device)\n",
    "\n",
    "            output = model(x_tensor)\n",
    "            output = output.squeeze(1)\n",
    "\n",
    "            loss = criterion(output, y_tensor)\n",
    "\n",
    "            prediction = torch.round(output)\n",
    "\n",
    "            accuracy += torch.sum(prediction == y_tensor).item()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            count += x_tensor.shape[0]\n",
    "        \n",
    "    return np.mean(losses), accuracy/count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20970354-b833-4738-8472-cd1da19b53c0",
   "metadata": {
    "id": "20970354-b833-4738-8472-cd1da19b53c0"
   },
   "source": [
    "Let's see how an untrained model performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c7410fb-acda-42a8-8c1f-d344b3dd07aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c7410fb-acda-42a8-8c1f-d344b3dd07aa",
    "outputId": "5dbf9ad6-5949-4995-bd63-fa5ff5f46300"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error before training: train loss 0.678 accuracy 0.599, validation loss 0.683 accuracy 0.544\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_accuracy = validation_step(model, x_train_batches, y_train_batches, criterion)\n",
    "val_loss, val_accuracy = validation_step(model, x_test_batches, y_test_batches, criterion)\n",
    "\n",
    "print(\"Error before training: train loss %.03f accuracy %.03f, validation loss %.03f accuracy %.03f\" % (train_loss, train_accuracy, val_loss, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8245fbd1-1096-43ef-93ca-6b97e76790a7",
   "metadata": {
    "id": "8245fbd1-1096-43ef-93ca-6b97e76790a7"
   },
   "source": [
    "Now, we try to train the model for a couple of iterations (so called epochs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44308f71-a27d-4ed2-b1f5-81c26b1ae80d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44308f71-a27d-4ed2-b1f5-81c26b1ae80d",
    "outputId": "bf2d422b-0b14-4f56-dc42-7dea1bc4f875"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 0.676 accuracy 0.607, validation loss 0.678 accuracy 0.559\n",
      "Epoch 101: train loss 0.635 accuracy 0.656, validation loss 0.683 accuracy 0.603\n",
      "Epoch 201: train loss 0.624 accuracy 0.656, validation loss 0.677 accuracy 0.603\n",
      "Epoch 301: train loss 0.604 accuracy 0.656, validation loss 0.659 accuracy 0.603\n",
      "Epoch 401: train loss 0.589 accuracy 0.691, validation loss 0.648 accuracy 0.632\n",
      "Epoch 501: train loss 0.577 accuracy 0.686, validation loss 0.646 accuracy 0.632\n",
      "Epoch 601: train loss 0.567 accuracy 0.721, validation loss 0.643 accuracy 0.647\n",
      "Epoch 701: train loss 0.562 accuracy 0.733, validation loss 0.641 accuracy 0.647\n",
      "Epoch 801: train loss 0.558 accuracy 0.731, validation loss 0.640 accuracy 0.647\n",
      "Epoch 901: train loss 0.556 accuracy 0.733, validation loss 0.639 accuracy 0.647\n",
      "Epoch 1001: train loss 0.554 accuracy 0.731, validation loss 0.639 accuracy 0.647\n",
      "Epoch 1101: train loss 0.552 accuracy 0.731, validation loss 0.639 accuracy 0.662\n",
      "Epoch 1201: train loss 0.551 accuracy 0.731, validation loss 0.638 accuracy 0.662\n",
      "Epoch 1301: train loss 0.550 accuracy 0.729, validation loss 0.636 accuracy 0.662\n",
      "Epoch 1401: train loss 0.549 accuracy 0.730, validation loss 0.632 accuracy 0.676\n",
      "Epoch 1501: train loss 0.548 accuracy 0.727, validation loss 0.631 accuracy 0.676\n",
      "Epoch 1601: train loss 0.547 accuracy 0.727, validation loss 0.631 accuracy 0.676\n",
      "Epoch 1701: train loss 0.546 accuracy 0.730, validation loss 0.632 accuracy 0.676\n",
      "Epoch 1801: train loss 0.546 accuracy 0.731, validation loss 0.632 accuracy 0.676\n",
      "Epoch 1901: train loss 0.545 accuracy 0.731, validation loss 0.632 accuracy 0.676\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2000\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    train_loss, train_accuracy = training_step(model, x_train_batches, y_train_batches, criterion, optimizer)\n",
    "    val_loss, val_accuracy = validation_step(model, x_test_batches, y_test_batches, criterion)\n",
    "    if e % 100 == 0:\n",
    "        print(\"Epoch %d: train loss %.03f accuracy %.03f, validation loss %.03f accuracy %.03f\" % (e + 1, train_loss, train_accuracy, val_loss, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f49c16-46bf-49f7-81e0-c95626294ca9",
   "metadata": {},
   "source": [
    "Try out changing a few parameters/adjusting the architecture!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe743f3-c76b-4c1c-8f31-cc898076586f",
   "metadata": {
    "id": "1fe743f3-c76b-4c1c-8f31-cc898076586f"
   },
   "source": [
    "## <center><font color=navy>Image processing example</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700a4821-6d98-48c6-91ef-750b56fce4dd",
   "metadata": {
    "id": "700a4821-6d98-48c6-91ef-750b56fce4dd"
   },
   "source": [
    "As a next example, we will attempted to classify handwritten digits from the MNIST dataset. Luckily, pytorch already provides functionalities for directly loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47bd63e8-9349-41e9-9571-41552fed69de",
   "metadata": {
    "id": "47bd63e8-9349-41e9-9571-41552fed69de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100 # The DataLoader will already partition the dataset into batches for us!\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38ba4d5e-c632-486e-b05a-20e6c72f8702",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "38ba4d5e-c632-486e-b05a-20e6c72f8702",
    "outputId": "dc05d434-2a2c-4624-ade9-8b5aa1172c59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28) torch.Size([100, 1, 28, 28])\n",
      "Label tensor(5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for data, target in train_loader:\n",
    "    img = data[0].numpy()\n",
    "    print(img.shape, data.shape)\n",
    "    plt.imshow(img[0])\n",
    "    print(\"Label\", target[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179faf22-b4c7-4605-8266-2c1dd240034a",
   "metadata": {
    "id": "179faf22-b4c7-4605-8266-2c1dd240034a"
   },
   "source": [
    "Next, we try to train a convolutional neural network (CNN) to recognize digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d53ce499-8404-4d01-ada3-24da477106c3",
   "metadata": {
    "id": "d53ce499-8404-4d01-ada3-24da477106c3"
   },
   "outputs": [],
   "source": [
    "class Network_MNIST_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network_MNIST_CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(2880, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)  #50x1x28x28 => 50x20x24x24\n",
    "        x = F.max_pool2d(x, 2) #pool: 50x20x24x24 => 50x20x12x12\n",
    "        x = torch.sigmoid(x)\n",
    "        x = x.view(-1, 2880) #50x20x12x12 => 50x2880\n",
    "        x = self.fc1(x) #50x2880 => 50x10\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf7eb49-5e44-4afb-873e-055c542d1176",
   "metadata": {
    "id": "ddf7eb49-5e44-4afb-873e-055c542d1176"
   },
   "source": [
    "Again, we setup the model, the cost function and the optimizer. Because we have more than two outputs, we use categorical cross-entropy. Note: In Pytorch, the function for categorical cross-entropy already includes the softmax, so no need to include it again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "325bedf6-45f7-4f6c-9348-c39697657944",
   "metadata": {
    "id": "325bedf6-45f7-4f6c-9348-c39697657944"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = Network_MNIST_CNN()\n",
    "model = model.to(device)\n",
    "\n",
    "learning_rate = 1e-1#\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc46954-53c2-4f91-953b-d7364bd8c454",
   "metadata": {
    "id": "adc46954-53c2-4f91-953b-d7364bd8c454"
   },
   "source": [
    "We again create two functions for training and for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a06b9a6-4847-4f03-a556-df1d32173855",
   "metadata": {
    "id": "3a06b9a6-4847-4f03-a556-df1d32173855"
   },
   "outputs": [],
   "source": [
    "def training_step(model, dataloader, loss, optimizer):\n",
    "    model.train()\n",
    "    accuracy = 0\n",
    "    losses = []\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for data, target in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x_tensor = data.to(device)\n",
    "        y_tensor = target.to(device)\n",
    "        \n",
    "        output = model(x_tensor)\n",
    "        output = output.squeeze(1)\n",
    "        \n",
    "        loss = criterion(output, y_tensor)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        prediction = torch.argmax(output, dim=1)\n",
    "        \n",
    "        accuracy += torch.sum(prediction == y_tensor).item()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        count += x_tensor.shape[0]\n",
    "        \n",
    "    return np.mean(losses), accuracy/count\n",
    "\n",
    "@torch.no_grad()    \n",
    "def validation_step(model, dataloader, loss):\n",
    "    model = model.eval()\n",
    "    \n",
    "    losses = []\n",
    "    accuracy = []\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        output = model(x)\n",
    "        output = output.squeeze(1)\n",
    "        losses.append(loss(output, y))\n",
    "    \n",
    "        prediction = torch.round(output)\n",
    "        acc = torch.sum(prediction == y).item()\n",
    "        accuracy.append(acc)\n",
    "        \n",
    "    return sum(losses)/len(losses), sum(accuracy)/len(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfd0286-9882-43cd-889c-604c6ca9806c",
   "metadata": {
    "id": "5dfd0286-9882-43cd-889c-604c6ca9806c"
   },
   "source": [
    "We then train the model (note: can be slow on CPU!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2b11212-5a92-485c-8778-6c6fd3328aea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2b11212-5a92-485c-8778-6c6fd3328aea",
    "outputId": "f3e65747-61b0-49eb-ac6c-2a454fcf24b2"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (100) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      4\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m training_step(model, train_loader, criterion, optimizer)\n\u001b[0;32m----> 5\u001b[0m     val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m%.03f\u001b[39;00m\u001b[38;5;124m accuracy \u001b[39m\u001b[38;5;132;01m%.03f\u001b[39;00m\u001b[38;5;124m, validation loss \u001b[39m\u001b[38;5;132;01m%.03f\u001b[39;00m\u001b[38;5;124m accuracy \u001b[39m\u001b[38;5;132;01m%.03f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, train_loss, train_accuracy, val_loss, val_accuracy))\n",
      "File \u001b[0;32m~/Desktop/CuRC/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 47\u001b[0m, in \u001b[0;36mvalidation_step\u001b[0;34m(model, dataloader, loss)\u001b[0m\n\u001b[1;32m     44\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss(output, y))\n\u001b[1;32m     46\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mround(output)\n\u001b[0;32m---> 47\u001b[0m     acc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[43mprediction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     48\u001b[0m     accuracy\u001b[38;5;241m.\u001b[39mappend(acc)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(losses)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(losses), \u001b[38;5;28msum\u001b[39m(accuracy)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(accuracy)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (100) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    train_loss, train_accuracy = training_step(model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_accuracy = validation_step(model, test_loader, criterion)\n",
    "    if e % 100 == 0:\n",
    "        print(\"Epoch %d: train loss %.03f accuracy %.03f, validation loss %.03f accuracy %.03f\" % (e + 1, train_loss, train_accuracy, val_loss, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c736cc-9688-4870-bdc6-3d3b48243c0e",
   "metadata": {},
   "source": [
    "Try adjusting the architecture/parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6effe039-ec1a-4002-bf9f-537ae49fa312",
   "metadata": {
    "id": "6effe039-ec1a-4002-bf9f-537ae49fa312"
   },
   "source": [
    "Let's try to create a fully-connected network that solves the same problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31ec204-2480-44aa-9d84-822285e7cf7a",
   "metadata": {
    "id": "b31ec204-2480-44aa-9d84-822285e7cf7a"
   },
   "outputs": [],
   "source": [
    "class Network_MNIST_FC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network_MNIST_FC, self).__init__()\n",
    "        #TODO\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784) #50x1x28x28 => 50x784\n",
    "        #TODO\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493249d5-d819-4bb5-9eda-d549c8873847",
   "metadata": {
    "id": "493249d5-d819-4bb5-9eda-d549c8873847"
   },
   "source": [
    "And then train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d6c662f-f7e6-4996-96db-97d763e5c109",
   "metadata": {
    "id": "9d6c662f-f7e6-4996-96db-97d763e5c109"
   },
   "outputs": [],
   "source": [
    "#TODO Train model! Try out changing parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kSKjWvhvEJkZ",
   "metadata": {
    "id": "kSKjWvhvEJkZ"
   },
   "source": [
    "## <center><font color=navy>Segmentation example</font></center>\n",
    "We will use a pretrained segmentation network (DeepLabV3) for liver segmentation and apply it to some images. First we initialize the model and load the pretrained parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YdvundG0EeMY",
   "metadata": {
    "id": "YdvundG0EeMY"
   },
   "outputs": [],
   "source": [
    "model = models.segmentation.deeplabv3_resnet50(pretrained=False, progress=True)\n",
    "model.classifier = DeepLabHead(2048, 2)\n",
    "\n",
    "model.load_state_dict(torch.load(\"model0049.th\"), strict=False)\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nOzsXcQxFfGf",
   "metadata": {
    "id": "nOzsXcQxFfGf"
   },
   "source": [
    "We then load an image from drive, preprocesses it and convert it into a pytorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K_l5tA3BFkNV",
   "metadata": {
    "id": "K_l5tA3BFkNV"
   },
   "outputs": [],
   "source": [
    "image_size = (640, 512)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose(\n",
    "     [transforms.ToTensor(),\n",
    "     normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jVTgC_ApF7zE",
   "metadata": {
    "id": "jVTgC_ApF7zE"
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(\"img1.png\")\n",
    "img = cv2.resize(img, image_size)\n",
    "\n",
    "img_tensor = transform(img).to(device)\n",
    "img_tensor = img_tensor.unsqueeze(0)\n",
    "\n",
    "plt.imshow(img[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KGfqbWQXG38p",
   "metadata": {
    "id": "KGfqbWQXG38p"
   },
   "source": [
    "We then feed the data into the model and visualize the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OH31j43pHQSi",
   "metadata": {
    "id": "OH31j43pHQSi"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  outputs = model(img_tensor)\n",
    "\n",
    "  pred = torch.argmax(outputs['out'], 1)\n",
    "\n",
    "pred_numpy = pred.cpu().numpy()\n",
    "\n",
    "plt.imshow(pred_numpy[0])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
