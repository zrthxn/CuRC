{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "269a5b6c",
   "metadata": {},
   "source": [
    "### <center><font color=navy> Tutorial #12 Computer- and robot-assisted surgery</font></center>\n",
    "## <center><font color=navy> ML Basics IV/Generative Adversarial Networks (GANs)</font></center>\n",
    "<center>&copy; Sebastian Bodenstedt & Micha Pfeiffer, National Center for Tumor Diseases (NCT) Dresden<br>\n",
    "    <a href=\"https://www.nct-dresden.de/\"><img src=\"https://www.nct-dresden.de/++theme++nct/images/logo-nct-en.svg\"></a> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed55bb0",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07595c16",
   "metadata": {},
   "source": [
    "Welcome to the lab focusing on *creating realistic synthetic surgical images with a sim2real Generative Adversarial Network (GAN)*. In this lab, we will generate images from synthetic surgical scenes using 3D models. We will then utilize a Generative Adversarial Network (GAN) to make these images look realistic. Furthermore, we will experiment with different styles and their effect on the output. The model used in this lab is based on the paper [*Generating large labeled data sets for\n",
    "laparoscopic image processing tasks using\n",
    "unpaired image-to-image translation*](https://arxiv.org/pdf/1907.02882.pdf) by Micha Pfeiffer et al.\n",
    "![Network](http://sds.bodenstedt.eu/Network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96360c79-663e-4f88-9521-107437d67806",
   "metadata": {},
   "source": [
    "## Download and extract models and style images\n",
    "\n",
    "To get started, we check if we already have downloaded the data archive, if not we download it a new from the internet and extract it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a288c5-1af8-4326-a76f-706b3e782f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from os.path import exists\n",
    "import zipfile\n",
    "import progressbar\n",
    "\n",
    "pbar = None\n",
    "\n",
    "def show_progress(block_num, block_size, total_size):\n",
    "    global pbar\n",
    "    if pbar is None:\n",
    "        pbar = progressbar.ProgressBar(maxval=total_size)\n",
    "        pbar.start()\n",
    "\n",
    "    downloaded = block_num * block_size\n",
    "    if downloaded < total_size:\n",
    "        pbar.update(downloaded)\n",
    "    else:\n",
    "        pbar.finish()\n",
    "        pbar = None\n",
    "\n",
    "url = \"http://sds.bodenstedt.eu/data.zip\"\n",
    "file_path = \"data.zip\"\n",
    "\n",
    "if not exists(file_path): # does zip file already exist?\n",
    "    urllib.request.urlretrieve(url, file_path, show_progress) # if not, download it\n",
    "    with zipfile.ZipFile(file_path, 'r') as zip_ref: # and unzip it\n",
    "        zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a253d1",
   "metadata": {},
   "source": [
    "## Load data into Python\n",
    "The aim of this task is load the relevant data and models into Python and, via pre-processing, make the data ready for style transfer.\n",
    "\n",
    "**Set up imports**\n",
    "\n",
    "First we import all relevant dependencies into Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b34f044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import cv2\n",
    "import sys\n",
    "import torch\n",
    "from image2image.networks import AdaINGen, StylelessGen\n",
    "import numpy as np\n",
    "import image2image.helper as helper\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d660e3de",
   "metadata": {},
   "source": [
    "**Loading models into Python**\n",
    "\n",
    "Then we load the file containing the parameters of the pre-trained encoders and decoders into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff534e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = torch.load(\"Model/Image2Image.pt\") # Load pre-trained models from file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31005df",
   "metadata": {},
   "source": [
    "Next, we instantiate the styleless encoder/decoder combination, feed it the pre-trained parameters and copy it to copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8e7733",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = StylelessGen(3, helper.param_model) # Instantiate Styleless encoder/decoder\n",
    "model_a.load_state_dict(models['a']) # Load pre-trained parameters\n",
    "model_a.cuda() # Move model to GPU\n",
    "model_a = model_a.eval() # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63abe979",
   "metadata": {},
   "source": [
    "We then perform the same steps with the AdaIN encoder/decoder combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a55549",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b = AdaINGen(3, helper.param_model) # Instantiate AdaIN encoder/decoder\n",
    "model_b.load_state_dict(models['b']) # Load pre-trained parameters\n",
    "model_b.cuda() # Move model to GPU\n",
    "mobel_b = model_b.eval() # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a5e66d",
   "metadata": {},
   "source": [
    "**Loading images into Pytorch**\n",
    "\n",
    "Next, we use OpenCV to load an image of the synthetic that we want to apply a style to. Furthermore, we load an image that contains the style that we want to apply. Here you can experiment with loading your own images (or different ones from the provided data, it contains 652 images) and/or with loading different style images from different patients. The dataset contains 5 images from four patients (P1-P4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"images/img010.png\") # Load synthetic image from file\n",
    "style_image = cv2.imread(\"styles/P1/style0.png\") # Load style image from file\n",
    "\n",
    "# Display images\n",
    "plt.xticks([]), plt.yticks([])  # Hides the graph ticks and x / y axis\n",
    "plt.imshow(image[:,:,::-1])\n",
    "plt.title(\"Synthetic Image\")\n",
    "plt.show()\n",
    "\n",
    "plt.xticks([]), plt.yticks([])  # Hides the graph ticks and x / y axis\n",
    "plt.imshow(style_image[:,:,::-1])\n",
    "plt.title(\"Style Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0da6ef",
   "metadata": {},
   "source": [
    "**Image pre-processing**\n",
    "\n",
    "In order to feed the images into the Pytorch networks, they need to be pre-processed first. For this, we write a function that first resizes the input image onto a size that is manageable for the GPU, then we convert the image from BGR-color notation (OpenCV Standard) to RGB-color notation. The image is then converted into a Pytorch tensor. The resulting tensor is then normalized to the range -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37b56f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_in, out_size):\n",
    "    image = cv2.resize(image_in, out_size, interpolation=cv2.INTER_LINEAR) # Resize image to wanted size\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Convert from BGR to RGB\n",
    "\n",
    "    img = transforms.functional.to_tensor(image) # Convert to Pytorch tensor\n",
    "    img = transforms.functional.normalize(img, (0.5,0.5,0.5), (0.5,0.5,0.5) ) # Normalize tensor to range -1 .. 1\n",
    "    img = img.unsqueeze(0) # add batch dimension\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acfe7b6-9d99-4595-9032-dc16a3746dcd",
   "metadata": {},
   "source": [
    "The two images are then pre-processed. As image size, we used a resolution of 580x270 pixels. Afterwards, we transfer the images to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c31d626-a9a3-412f-9349-e380859dc969",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (580, 270) # Network input resolution\n",
    "\n",
    "# Pre-process images\n",
    "image_tensor = preprocess_image(image, input_size)\n",
    "style_image_tensor = preprocess_image(style_image, input_size)\n",
    "\n",
    "# Move image data to GPU\n",
    "image_tensor = image_tensor.cuda()\n",
    "style_image_tensor = style_image_tensor.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd5f352-c8c6-4213-b080-76e5aaa9c647",
   "metadata": {},
   "source": [
    "## Apply style transfer to image\n",
    "Now that we have loaded the required neural networks as well as loaded and pre-processed the images, we can now continue with performing the style transfer. \n",
    "\n",
    "**Compact representation of the synthetic image**\n",
    "\n",
    "First, using the encoder of the pre-trained styleless network, we calculate the compact representation of the synthetic image. ![Encoder](http://sds.bodenstedt.eu/t51.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa081249-219e-48ab-a25c-5598b4765cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): # Since we are not training the network, we don't need gradient information to be retained\n",
    "    image_representation = model_a.encode(image_tensor) # Calculate compact representation for synthetic image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3801be-17e0-418c-86cb-d1f0595626dc",
   "metadata": {},
   "source": [
    "**Extract/generate style**\n",
    "\n",
    "Using the pre-trained AdaIN network, we calculate the compact representation of the style image as well as the compact representation of the style. As we are only interested in the latter, we discard the former. ![Style encoder](http://sds.bodenstedt.eu/t52_1.png)![Random style](http://sds.bodenstedt.eu/t52_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf82833c-9b5a-429f-a27c-fbe52a6d7efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    _, style = model_b.encode(style_image_tensor) # Extract style from style image\n",
    "    print(\"Extracted style vector:\")\n",
    "    for i in range(helper.param_model[\"style_dim\"]):\n",
    "        print(style[0, i, 0, 0].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f384d7-b9f9-453d-a1e3-9c3c8575501d",
   "metadata": {},
   "source": [
    "Alternatively, we can also generate a random vector, representing a random style. For this, we draw from a standard normal distribution (mean 0, variance 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7924ab3-9b9e-4fbd-a119-d288662bd2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_random = torch.randn(1, helper.param_model[\"style_dim\"], 1, 1) # Generate random style vector\n",
    "print(\"Random style vector:\")\n",
    "for i in range(helper.param_model[\"style_dim\"]):\n",
    "    # style vector can be manually adjusted by assigning values to the different entries\n",
    "    #style_random[0, i, 0, 0] = 0\n",
    "    print(style_random[0, i, 0, 0].item())\n",
    "style_random = style_random.cuda() # Move vector to GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab65bd15-26ad-4a26-a48f-160d682e724e",
   "metadata": {},
   "source": [
    "**Apply style to synthetic image**\n",
    "\n",
    "Next, we aim to apply a style (either the extracted style or a random style) to the compact representation of the synthetic image. For this, we feed both the compact representation and a style vector into the decoder of the AdaIN network.\n",
    "![Decoder](http://sds.bodenstedt.eu/t53.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c15d26a-6384-4b8f-a7fe-91fe99c347b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    stylized_image = model_b.decode(image_representation, style) # Apply extracted style to compact representation of synthetic image\n",
    "    \n",
    "    stylized_image_random = model_b.decode(image_representation, style_random) # Apply randomly generated style to compact representation of synthetic image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b640b6-f495-4b83-a82e-c74756d31ba6",
   "metadata": {},
   "source": [
    "**Visualize results**\n",
    "\n",
    "To visualize the resulting images, we first need to convert them back into a standard image format (e.g. OpenCV/NumPy). For this, we write a function that first removes the extraneous batch dimension. We then reverse the normalization, converting the pixel values to a range of 0 to 1. The result is then copied back into RAM and converted into a NumPy data structure. We then rearrange the dimension into $height \\times width \\times channels$ and scale the pixel values to a range of 0 to 255. The image is then converted into a 8-bit data format and we convert it from RGB to BGR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9467fc0-e0a4-4739-ad8e-4f6ddefd7911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image(image_in):\n",
    "    image = image_in.squeeze(0) # Remove batch dimension\n",
    "    image = transforms.functional.normalize(image, (-1,-1,-1), (2,2,2) ) # adjust to range 0 .. 1\n",
    "    image = image.cpu().numpy() # Copy data to CPU and convert to NumPy\n",
    "\n",
    "    image = np.transpose(image, (1, 2, 0))*255 # Adjust order of the dimensions and scale to 0 .. 255\n",
    "    image = image.astype(np.uint8) # Convert to 8 bit data type\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # convert RGB to BGR\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3586b165-d44a-415c-8f0a-5e7c19ed7163",
   "metadata": {},
   "source": [
    "We apply the function to the generated images and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c027aec-c782-4b09-8be2-5531963e3ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert stylized images to OpenCV images\n",
    "stylized_image_cv = convert_image(stylized_image) \n",
    "stylized_image_random_cv = convert_image(stylized_image_random)\n",
    "\n",
    "# Visualize results\n",
    "plt.xticks([]), plt.yticks([])  # Hides the graph ticks and x / y axis\n",
    "plt.imshow(image[:,:,::-1])\n",
    "plt.title(\"Synthetic Image\")\n",
    "plt.show()\n",
    "\n",
    "plt.xticks([]), plt.yticks([])  # Hides the graph ticks and x / y axis\n",
    "plt.imshow(style_image[:,:,::-1])\n",
    "plt.title(\"Style Image\")\n",
    "plt.show()\n",
    "\n",
    "plt.xticks([]), plt.yticks([])  # Hides the graph ticks and x / y axis\n",
    "plt.imshow(stylized_image_cv[:,:,::-1])\n",
    "plt.title(\"Synthetic Image with extracted style\")\n",
    "plt.show()\n",
    "\n",
    "plt.xticks([]), plt.yticks([])  # Hides the graph ticks and x / y axis\n",
    "plt.imshow(stylized_image_random_cv[:,:,::-1])\n",
    "plt.title(\"Synthetic Image with random style\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc2de5f-6180-41ee-b269-65bc9e327301",
   "metadata": {},
   "source": [
    "## Experiment with different styles\n",
    "Now that we have setup an entire pipeline for extracting styles/generating random styles and applying those styles to synthetic images, we can experiment with extracting different styles. For this, try modifying which images are loaded for styles extraction. Also consider adding your own images (don't have to be surgery related). Alternative you can also manually modify the style. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
